{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7127b62",
   "metadata": {},
   "source": [
    "# Prática spark\n",
    "## Objetivo\n",
    "El objetivo de esta práctica es realizar el procesado de un fichero de logs para poder\n",
    "## Práctica\n",
    "### Conexión\n",
    "El primer paso para realizar la práctica es establecer la conexión con el cluster de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a1c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/21 17:14:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from  pyspark.sql.functions import col, concat_ws,split, explode ,desc, to_timestamp,from_unixtime,unix_timestamp, translate, create_map, lit,udf,StringType,map_keys\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d03b2",
   "metadata": {},
   "source": [
    "### Lectura\n",
    "Una vez que se ha establecido la conexión con el cluster de spark, es necesario cargar el archivo de texto que se va a tratar.\n",
    "Para esta práctica se va a leer como un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65d3394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               datos|\n",
      "+--------------------+\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "|01-11-2022 13:24:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ficheroDataframe = spark.read.text(\"logs.txt\")\n",
    "ficheroDataFrameRenamed = ficheroDataframe.withColumnRenamed(\"value\",\"datos\")\n",
    "# Se muestran los primeros 20 elementos\n",
    "ficheroDataFrameRenamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61976550",
   "metadata": {},
   "source": [
    "### Preprocesado\n",
    "\n",
    "Una vez que se ha cargado el fichero, será necesario dividir la columna por el carácter \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56254b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|datos|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ficheroDataFrameRenamed.filter(col(\"datos\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbe8d3",
   "metadata": {},
   "source": [
    "Además comprobamos que todas las filas tienen el carácter por el que vamos a separar el string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819348af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|datos|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ficheroDataFrameRenamed.filter(~ col(\"Datos\").contains(\" - \")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac83f5d",
   "metadata": {},
   "source": [
    "Se eliminan los valores Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfeec5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ficheroDataFrameRenamed = ficheroDataFrameRenamed.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0f8e9",
   "metadata": {},
   "source": [
    "Ahora que ya se sabe que los datos de entrada tienen el formato adecuado se puede pasar a la división"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1052b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ficheroDataFrameSplit = ficheroDataFrameRenamed.select(split(col(\"datos\"),\" - \"))\n",
    "ficheroDataFrameSplit = ficheroDataFrameSplit.withColumnRenamed(\"split(datos,  - , -1)\",\"datos\")\n",
    "\n",
    "ficheroDataFrameSplit.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04187860",
   "metadata": {},
   "source": [
    "Se construye el dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395b1b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+\n",
      "|              Fecha|     Log|Module|                Info|\n",
      "+-------------------+--------+------+--------------------+\n",
      "|01-11-2022 13:24:13|    INFO|pandas|Aliquam amet adip...|\n",
      "|01-11-2022 13:24:17| WARNING| spark|Consectetur velit...|\n",
      "|01-11-2022 13:24:20| WARNING|pandas|Porro etincidunt ...|\n",
      "|01-11-2022 13:24:22|   ERROR|pandas|Tempora modi quiq...|\n",
      "|01-11-2022 13:24:24| WARNING| spark|Quiquia etincidun...|\n",
      "|01-11-2022 13:24:25|    INFO| spark|Non est porro por...|\n",
      "|01-11-2022 13:24:26|CRITICAL|python|Porro labore eius...|\n",
      "|01-11-2022 13:24:28|CRITICAL| numpy|Est ut tempora se...|\n",
      "|01-11-2022 13:24:29|    INFO| spark|Sit dolorem dolor...|\n",
      "|01-11-2022 13:24:30|CRITICAL| spark|Etincidunt aliqua...|\n",
      "|01-11-2022 13:24:31|   DEBUG|python|Aliquam sed porro...|\n",
      "|01-11-2022 13:24:32|CRITICAL| numpy|Numquam dolor ips...|\n",
      "|01-11-2022 13:24:33|CRITICAL|python|Amet neque est ip...|\n",
      "|01-11-2022 13:24:34|   DEBUG|python|Eius ut tempora i...|\n",
      "|01-11-2022 13:24:35| WARNING| spark|Quiquia numquam s...|\n",
      "|01-11-2022 13:24:36|   ERROR| numpy|Quisquam adipisci...|\n",
      "|01-11-2022 13:24:37|CRITICAL| spark|Voluptatem dolor ...|\n",
      "|01-11-2022 13:24:38|CRITICAL| spark| Dolor neque ut non.|\n",
      "|01-11-2022 13:24:39|   DEBUG| numpy|Tempora adipisci ...|\n",
      "|01-11-2022 13:24:40|   ERROR| numpy|Velit numquam dol...|\n",
      "+-------------------+--------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameDatos= ficheroDataFrameSplit.select(col(\"datos\").getItem(0),col(\"datos\").getItem(1),col(\"datos\").getItem(2),col(\"datos\").getItem(3))\n",
    "# Se renombran las columnas\n",
    "dataFrameDatos = dataFrameDatos.withColumnRenamed(\"datos[0]\",\"Fecha\")\n",
    "dataFrameDatos = dataFrameDatos.withColumnRenamed(\"datos[1]\",\"Log\")\n",
    "dataFrameDatos = dataFrameDatos.withColumnRenamed(\"datos[2]\",\"Module\")\n",
    "dataFrameDatos = dataFrameDatos.withColumnRenamed(\"datos[3]\",\"Info\")\n",
    "dataFrameDatos.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e23a",
   "metadata": {},
   "source": [
    "Ahora se tiene el dataframe con los datos separados por columnas, se tiene que transformar la fecha a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b54494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+\n",
      "|              Fecha|     Log|Module|                Info|\n",
      "+-------------------+--------+------+--------------------+\n",
      "|2022-11-01 13:24:13|    INFO|pandas|Aliquam amet adip...|\n",
      "|2022-11-01 13:24:17| WARNING| spark|Consectetur velit...|\n",
      "|2022-11-01 13:24:20| WARNING|pandas|Porro etincidunt ...|\n",
      "|2022-11-01 13:24:22|   ERROR|pandas|Tempora modi quiq...|\n",
      "|2022-11-01 13:24:24| WARNING| spark|Quiquia etincidun...|\n",
      "|2022-11-01 13:24:25|    INFO| spark|Non est porro por...|\n",
      "|2022-11-01 13:24:26|CRITICAL|python|Porro labore eius...|\n",
      "|2022-11-01 13:24:28|CRITICAL| numpy|Est ut tempora se...|\n",
      "|2022-11-01 13:24:29|    INFO| spark|Sit dolorem dolor...|\n",
      "|2022-11-01 13:24:30|CRITICAL| spark|Etincidunt aliqua...|\n",
      "|2022-11-01 13:24:31|   DEBUG|python|Aliquam sed porro...|\n",
      "|2022-11-01 13:24:32|CRITICAL| numpy|Numquam dolor ips...|\n",
      "|2022-11-01 13:24:33|CRITICAL|python|Amet neque est ip...|\n",
      "|2022-11-01 13:24:34|   DEBUG|python|Eius ut tempora i...|\n",
      "|2022-11-01 13:24:35| WARNING| spark|Quiquia numquam s...|\n",
      "|2022-11-01 13:24:36|   ERROR| numpy|Quisquam adipisci...|\n",
      "|2022-11-01 13:24:37|CRITICAL| spark|Voluptatem dolor ...|\n",
      "|2022-11-01 13:24:38|CRITICAL| spark| Dolor neque ut non.|\n",
      "|2022-11-01 13:24:39|   DEBUG| numpy|Tempora adipisci ...|\n",
      "|2022-11-01 13:24:40|   ERROR| numpy|Velit numquam dol...|\n",
      "+-------------------+--------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Fecha: timestamp (nullable = true)\n",
      " |-- Log: string (nullable = true)\n",
      " |-- Module: string (nullable = true)\n",
      " |-- Info: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameDatos = dataFrameDatos.withColumn(\"Fecha\",to_timestamp(col(\"Fecha\"),\"dd-MM-yyyy HH:mm:ss\"))\n",
    "dataFrameDatos.show()\n",
    "dataFrameDatos.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01429f0f",
   "metadata": {},
   "source": [
    "Por último va a ser necesario realilzar un mapeo entre el nivel de incidencia y in valor numérico previamente establecido, pero antes se va a comprobar que todas las filas tienen un string esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82c54fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+---+\n",
      "|              Fecha|     Log|Module|                Info|Map|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "|2022-11-01 13:24:13|    INFO|pandas|Aliquam amet adip...| 20|\n",
      "|2022-11-01 13:24:17| WARNING| spark|Consectetur velit...| 30|\n",
      "|2022-11-01 13:24:20| WARNING|pandas|Porro etincidunt ...| 30|\n",
      "|2022-11-01 13:24:22|   ERROR|pandas|Tempora modi quiq...| 40|\n",
      "|2022-11-01 13:24:24| WARNING| spark|Quiquia etincidun...| 30|\n",
      "|2022-11-01 13:24:25|    INFO| spark|Non est porro por...| 20|\n",
      "|2022-11-01 13:24:26|CRITICAL|python|Porro labore eius...| 50|\n",
      "|2022-11-01 13:24:28|CRITICAL| numpy|Est ut tempora se...| 50|\n",
      "|2022-11-01 13:24:29|    INFO| spark|Sit dolorem dolor...| 20|\n",
      "|2022-11-01 13:24:30|CRITICAL| spark|Etincidunt aliqua...| 50|\n",
      "|2022-11-01 13:24:31|   DEBUG|python|Aliquam sed porro...| 10|\n",
      "|2022-11-01 13:24:32|CRITICAL| numpy|Numquam dolor ips...| 50|\n",
      "|2022-11-01 13:24:33|CRITICAL|python|Amet neque est ip...| 50|\n",
      "|2022-11-01 13:24:34|   DEBUG|python|Eius ut tempora i...| 10|\n",
      "|2022-11-01 13:24:35| WARNING| spark|Quiquia numquam s...| 30|\n",
      "|2022-11-01 13:24:36|   ERROR| numpy|Quisquam adipisci...| 40|\n",
      "|2022-11-01 13:24:37|CRITICAL| spark|Voluptatem dolor ...| 50|\n",
      "|2022-11-01 13:24:38|CRITICAL| spark| Dolor neque ut non.| 50|\n",
      "|2022-11-01 13:24:39|   DEBUG| numpy|Tempora adipisci ...| 10|\n",
      "|2022-11-01 13:24:40|   ERROR| numpy|Velit numquam dol...| 40|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_mapping = {\n",
    "    'CRITICAL': 50,\n",
    "    'ERROR': 40,\n",
    "    'WARNING': 30,\n",
    "    'INFO': 20,\n",
    "    'DEBUG': 10, \n",
    "    'NOTSET': 0}\n",
    "log_mapping_spark = create_map([lit(x) for x in chain(*log_mapping.items())])\n",
    "\n",
    "lista_incidencias = ['CRITICAL','ERROR','WARNING','INFO','DEBUG','NOTSET']\n",
    "\n",
    "dataFrameDatos = dataFrameDatos.filter(col(\"Log\").isin(lista_incidencias))\n",
    "\n",
    "dataFrameDatos = dataFrameDatos.withColumn(\"Map\",log_mapping_spark[col(\"Log\")])\n",
    "dataFrameDatos.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6df12",
   "metadata": {},
   "source": [
    "Se comprueba que las columnas tienen el formato que esperamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37d2b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fecha: timestamp (nullable = true)\n",
      " |-- Log: string (nullable = true)\n",
      " |-- Module: string (nullable = true)\n",
      " |-- Info: string (nullable = true)\n",
      " |-- Map: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameDatos.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a002121",
   "metadata": {},
   "source": [
    "Como era de esperar no se ha encontrado ningún valor a null, por lo tanto se puede continuar con la práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567a0a3",
   "metadata": {},
   "source": [
    "### Filtrado por módulo\n",
    "Con el dataframe preprocesado como se desea, se va a comenzar a extraer información.\n",
    "En un inicio se van a extraer las filas que coincidan con un valor de módulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318bfd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+---+\n",
      "|              Fecha|     Log|Module|                Info|Map|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "|2022-11-01 13:24:17| WARNING| spark|Consectetur velit...| 30|\n",
      "|2022-11-01 13:24:24| WARNING| spark|Quiquia etincidun...| 30|\n",
      "|2022-11-01 13:24:25|    INFO| spark|Non est porro por...| 20|\n",
      "|2022-11-01 13:24:29|    INFO| spark|Sit dolorem dolor...| 20|\n",
      "|2022-11-01 13:24:30|CRITICAL| spark|Etincidunt aliqua...| 50|\n",
      "|2022-11-01 13:24:35| WARNING| spark|Quiquia numquam s...| 30|\n",
      "|2022-11-01 13:24:37|CRITICAL| spark|Voluptatem dolor ...| 50|\n",
      "|2022-11-01 13:24:38|CRITICAL| spark| Dolor neque ut non.| 50|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Modi est modi non...| 10|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Magnam numquam ut...| 30|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Modi quiquia cons...| 10|\n",
      "|2022-11-01 13:24:51|    INFO| spark|Neque adipisci ma...| 20|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Velit numquam neq...| 30|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Quaerat magnam se...| 10|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Est modi sit nequ...| 30|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Sit ipsum quiquia...| 10|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Quisquam amet adi...| 30|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro dolorem sit...| 40|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Dolor sed etincid...| 10|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Non magnam volupt...| 30|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modulo = \"spark\"\n",
    "dataFrameDatos = dataFrameDatos.filter(modulo== col(\"Module\"))\n",
    "dataFrameDatos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d1e56",
   "metadata": {},
   "source": [
    "### Filtrado por palabra en string\n",
    "En este apartado se va filtrar si una palabra se encuentra en el campo de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de23e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+---+\n",
      "|              Fecha|     Log|Module|                Info|Map|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro dolorem sit...| 40|\n",
      "|2022-11-01 13:24:51|CRITICAL| spark|Porro eius consec...| 50|\n",
      "|2022-11-01 13:24:51|   DEBUG| spark|Porro sed dolore ...| 10|\n",
      "|2022-11-01 13:24:51|    INFO| spark|Porro tempora qui...| 20|\n",
      "|2022-11-01 13:24:51|    INFO| spark|Porro dolorem dol...| 20|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro consectetur...| 40|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro est neque d...| 30|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro ut eius sed...| 40|\n",
      "|2022-11-01 13:24:51|    INFO| spark|Porro est numquam...| 20|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro quaerat ame...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro ut tempora ...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro dolorem con...| 30|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "palabra = \"Porro\"\n",
    "dataFrameDatos = dataFrameDatos.filter(col(\"Info\").contains(palabra))\n",
    "dataFrameDatos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273aa5f",
   "metadata": {},
   "source": [
    "### Filtrado por input de palabra\n",
    "Se van a mostrar las filas que tengan un nivel de incidencia igual o superior que el nivel solicitado. De esta forma si se introduce como nivel mínimo \"Warning\", se deberían de mostrar las filas con niveles \"Warning\", \"Error\" y \"Critical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6015eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+---+\n",
      "|              Fecha|     Log|Module|                Info|Map|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro dolorem sit...| 40|\n",
      "|2022-11-01 13:24:51|CRITICAL| spark|Porro eius consec...| 50|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro consectetur...| 40|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro est neque d...| 30|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro ut eius sed...| 40|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro quaerat ame...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro ut tempora ...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro dolorem con...| 30|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nivel = \"WARNING\"\n",
    "\n",
    "# Primero se obtiene el valor numérico para el nivel\n",
    "valor = log_mapping[nivel]\n",
    "\n",
    "dataFrameDatos = dataFrameDatos.filter(valor <= col(\"Map\"))\n",
    "dataFrameDatos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba4504",
   "metadata": {},
   "source": [
    "### Filtrado por fecha\n",
    "Se filtra para obtener todas las entradas del log posteriores al valor de fecha establecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89640729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+------+--------------------+---+\n",
      "|              Fecha|     Log|Module|                Info|Map|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro dolorem sit...| 40|\n",
      "|2022-11-01 13:24:51|CRITICAL| spark|Porro eius consec...| 50|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro consectetur...| 40|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro est neque d...| 30|\n",
      "|2022-11-01 13:24:51|   ERROR| spark|Porro ut eius sed...| 40|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro quaerat ame...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro ut tempora ...| 30|\n",
      "|2022-11-01 13:24:51| WARNING| spark|Porro dolorem con...| 30|\n",
      "+-------------------+--------+------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "after = datetime.strptime('11/01/22 13:24:41', '%m/%d/%y %H:%M:%S')\n",
    "\n",
    "dataFrameDatos = dataFrameDatos.filter(col(\"Fecha\") >(after)) \n",
    "dataFrameDatos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e734d6a",
   "metadata": {},
   "source": [
    "### Programación defensiva\n",
    "En esta práctica se han identificado problemas que harían que la ejecución fallara, pero no se ha sabido implementar las soluciones. Sería interesante protegerse frente a fallos en el formato de entrada, como por ejemplo que el carácter de separación no sea \"-\", aunque el alumno no ha sido capaz de implementarlo.\n",
    "También interesaría protegerse frente a fechas incorrectas, que estuvieran en el pasado o que no cumplieran con el formato establecido, pero una vez más el alumno no ha sabido implementarlo.\n",
    "Únicamente se ha protegido el comienzo del código para identificar las filas que pudieran estar vacías.\n",
    "Por último se ha conseguido implementar la comprobación de que el nivel de incidencia se encuentra en la lista de valores conocidos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
